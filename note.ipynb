{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Probing exemple (squelette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Charger le modèle pré-entraîné\n",
    "checkpoint = torch.load('checkpoint.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# 2. Geler l'encodeur (backbone)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 3. Ajouter une couche linéaire pour la classification\n",
    "class LinearProbingModel(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(LinearProbingModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc = nn.Linear(backbone.output_dim, num_classes)  # Projection vers le nombre de classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():  # Ne met pas à jour les poids de l'encodeur\n",
    "            features = self.backbone(x)\n",
    "        return self.fc(features)\n",
    "\n",
    "\n",
    "# 4. Entraîner la couche linéaire\n",
    "model = LinearProbingModel(backbone=model.encoder_q, num_classes=1000)  # Exemple avec 1000 classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Boucle d'entraînement pour le linear probing\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
